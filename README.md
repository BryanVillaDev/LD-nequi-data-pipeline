# LD-nequi-data-pipeline

## Propuesta para un Pipeline de Datos Moderno

### IntroducciÃ³n

Aca explico cÃ³mo montar un pipeline de datos utilizando aws y herramientas modernas escalables tambien buenas prÃ¡cticas en el diseÃ±o del codigo. La idea es contar con una soluciÃ³n integral que abarque desde la ingesta y transformaciÃ³n de datos hasta su validaciÃ³n y anÃ¡lisis, apoyÃ¡ndonos en herramientas como **AWS**, **Airflow** para orquestaciÃ³n, **AWS Glue (PySpark)** para procesamiento y **Terraform** para gestionar la infraestructura.

AdemÃ¡s, se implementa un enfoque de **Test-Driven Development (TDD)**, aplicando principios **SOLID** y de **Clean Architecture** para que el cÃ³digo sea modular, fÃ¡cil de mantener y escalable.

### Objetivos de la Propuesta

Con esta soluciÃ³n buscamos:

- **Automatizar** todo el ciclo de procesamiento de datos, desde la ingesta hasta la visualizaciÃ³n.
- **Garantizar la calidad y el versionado** de los datos usando **Apache Hudi**.
- **Facilitar el anÃ¡lisis** cargando los datos en un **data warehouse (Snowflake)** e integrÃ¡ndolos con herramientas de **BI**.
- **Asegurar la resiliencia** del pipeline mediante notificaciones automÃ¡ticas (por ejemplo, con **SES, Slack y Teams**) en caso de fallos.
- **Implementar un entorno de despliegue continuo** y validaciÃ³n de cÃ³digo con **Terraform y GitHub Actions**.

### Arquitectura y Flujo de Datos

El pipeline se estructura de la siguiente manera:

1. **IngestiÃ³n y Trigger**

   - Cargas archivos **CSV** en un **bucket de S3**, lo que dispara un **DAG en Airflow**.

2. **Procesamiento y Curado**

   - **Airflow** orquesta un **job en AWS Glue**, donde **PySpark** lee el **CSV**, aplica transformaciones y validaciones, y cura los datos usando **Apache Hudi**.
   - Los datos procesados se almacenan en **S3** y, opcionalmente, se cargan en **Snowflake** para anÃ¡lisis.

3. **Notificaciones**

   - Si se produce algÃºn error, se envÃ­a una notificaciÃ³n automÃ¡tica vÃ­a **email, Slack o Teams**.

4. **AnÃ¡lisis y VisualizaciÃ³n**
   - Los datos en **Snowflake** se integran con herramientas de **BI** como **QuickSight** o **Grafana** para facilitar su anÃ¡lisis y visualizaciÃ³n.

# Secuencia del Flujo

![secuence](image.png)

# Diagrama Arquitectura

![arquitectura](arquitetcura.svg)

---

## Estructura del Repositorio

La organizaciÃ³n de carpetas estÃ¡ diseÃ±ada para separar responsabilidades y facilitar el mantenimiento y la escalabilidad. La estructura es la siguiente:

La estructura de carpetas estÃ¡ diseÃ±ada para separar las responsabilidades y facilitar el mantenimiento y la escalabilidad. La organizaciÃ³n se realiza de la siguiente manera:

## Estructura del Proyecto

```bash
Proyecto/
â”œâ”€â”€ README.md              # Documento final (esta propuesta)
â”œâ”€â”€ docs/                  # DocumentaciÃ³n adicional (diagramas, presentaciones, etc.)
â”‚   â”œâ”€â”€ higlevel.png       # Diagrama de arquitectura high-level
â”‚   â”œâ”€â”€ secuencia.png      # Diagrama de secuencia del flujo de datos
â”‚   â””â”€â”€ modelo_operativo.md # Modelo operativo y plan de soporte
â”œâ”€â”€ airflow/               # CÃ³digo y configuraciones de Airflow
â”‚   â”œâ”€â”€ dags/              # DefiniciÃ³n de DAGs para la orquestaciÃ³n
â”‚   â”‚   â””â”€â”€ transactions_etl.py
â”‚   â”œâ”€â”€ tests/             # Tests unitarios/integraciÃ³n para los DAGs
â”‚   â”‚   â””â”€â”€ test_transactions_etl.py
â”‚   â””â”€â”€ plugins/           # Operadores y hooks personalizados (si es necesario)
â”œâ”€â”€ glue/                  # CÃ³digo para AWS Glue (PySpark)
â”‚   â”œâ”€â”€ scripts/           # Scripts de transformaciÃ³n y procesamiento
â”‚   â”‚   â””â”€â”€ script_pyspark.py
â”‚   â”œâ”€â”€ tests/             # Tests para validar la lÃ³gica de transformaciÃ³n en PySpark
â”‚   â”‚   â””â”€â”€ test_script_pyspark.py
â”œâ”€â”€ notifications/         # MÃ³dulo de notificaciones
â”‚   â”œâ”€â”€ notifier.py        # Clase Notifier con mÃ©todos para email, Slack y Teams
â”‚   â”œâ”€â”€ tests/             # Tests para el mÃ³dulo de notificaciones
â”‚   â”‚   â””â”€â”€ test_notifier.py
â”œâ”€â”€ terraform/             # ConfiguraciÃ³n de infraestructura como cÃ³digo
â”‚   â”œâ”€â”€ main.tf            # DefiniciÃ³n de recursos (S3, IAM, etc.)
â”‚   â”œâ”€â”€ variables.tf
â”‚   â”œâ”€â”€ outputs.tf
â”‚   â””â”€â”€ modules/           # MÃ³dulos reutilizables
â”œâ”€â”€ ci/                    # Configuraciones y scripts de CI/CD con GitHub Actions
â”‚   â””â”€â”€ workflows/         # Workflows YAML (por ejemplo, terraform.yml)
â””â”€â”€ data/                  # Archivos de datos de ejemplo (CSV)
    â””â”€â”€ transactions.csv
```

---

## Pipeline de CI/CD y GestiÃ³n de TDD

El proceso de integraciÃ³n y despliegue se automatiza con **GitHub Actions** mediante flujos de trabajo especÃ­ficos para cada componente:

- **Terraform**: ValidaciÃ³n, planificaciÃ³n y despliegue de la infraestructura.
- **Airflow**: ValidaciÃ³n y pruebas de los DAGs.
- **AWS Glue/PySpark**: EjecuciÃ³n de tests unitarios para confirmar las transformaciones de datos.
- **Notificaciones**: Pruebas del mÃ³dulo Notifier.

Cada pipeline se activa solo cuando se detectan cambios en las carpetas correspondientes, lo que garantiza una **integraciÃ³n continua sin afectar el entorno de producciÃ³n**.

### Almacenamiento de DAGs y Jobs

- Los **DAGs de Airflow** y los **scripts de transformaciÃ³n en PySpark** se almacenan en **buckets S3 especÃ­ficos por ambiente** (`dev`, `staging` y `prod`).
- Airflow y Glue acceden a estos scripts desde sus respectivos buckets para garantizar **versionado, aislamiento y seguridad**.
- La estructura en S3 es la siguiente:

```bash
s3://ld-poc-nequi-dev/dags/
s3://ld-poc-nequi-dev/jobs/
s3://ld-poc-nequi-staging/dags/
s3://ld-poc-nequi-staging/jobs/
s3://ld-poc-nequi-prod/dags/
s3://ld-poc-nequi-prod/jobs/
```

## propuesta para la estrategia de ramas Eficiente en un equipo colaborativo

![alt text](gitflow.png)

## propuesta para el pipline de deploy haciendo revisiÃ³n de codigo

![alt text](pullRequest.png)

---

## Principios de SOLID y Clean Architecture

- **SOLID**  
  La implementaciÃ³n del ETL en **PySpark** y el mÃ³dulo **Notifier** sigue un enfoque orientado a objetos, donde cada clase tiene una Ãºnica responsabilidad, mejorando la mantenibilidad y escalabilidad.

- **Clean Architecture**  
  La soluciÃ³n se organiza en **capas independientes** (orquestaciÃ³n, procesamiento, notificaciones e infraestructura), lo que evita que los cambios en un mÃ³dulo impacten a los demÃ¡s y facilita la evoluciÃ³n del proyecto.

---

## Ventajas de la Propuesta

- **Modularidad y Escalabilidad**:  
  La estructura del proyecto permite crecer de forma ordenada y controlada.

- **AutomatizaciÃ³n Integral**:  
  La combinaciÃ³n de **Terraform** y **GitHub Actions** asegura despliegues automÃ¡ticos, seguros y consistentes.

- **Calidad Asegurada con TDD**:  
  La ejecuciÃ³n de pruebas unitarias e integraciÃ³n permite detectar errores en etapas tempranas del desarrollo.

- **Flexibilidad y Configurabilidad**:  
  El uso de variables de entorno y parÃ¡metros configurables facilita la adaptaciÃ³n a distintos entornos sin necesidad de modificar el cÃ³digo base.

- **Buenas PrÃ¡cticas de Desarrollo**:  
  Aplicar los principios **SOLID** y **Clean Architecture** garantiza un cÃ³digo limpio, mantenible y fÃ¡cil de extender para futuras mejoras.

---

# Modelo Operativo

[Modelo Operativo](docs/modelo_operativo.md)



# GestiÃ³n del Pipeline con Scrum y PriorizaciÃ³n Eisenhower

Pa organizar bien el trabajo en el pipeline usamos **Scrum** (para que el equipo tenga claro que hacer en cada sprint) y la **Matriz de Eisenhower** (para no perder tiempo en cosas que no aportan valor real y enfocarnos en lo que de verdad importa).

## ğŸ”¹ **Scrum en el Pipeline**  

La idea es trabajar en **sprints de 2 semanas**, asi aseguramos que siempre hay avances sin meter cambios a lo loco que rompan todo.

### **ğŸ”¹ Roles en Scrum:**  
- **Product Owner (PO)**: Define que se hace en cada sprint y que es lo mÃ¡s importante pa el negocio.  
- **LÃ­der TÃ©cnico / LÃ­der de Datos**:  
  - Se encarga de que la arquitectura y el cÃ³digo sean solidos.  
  - Revisa Pull Requests y asegura que todo pase los tests y validaciones.  
  - Evita que se hagan cosas sin sentido y que el equipo no pierda tiempo en tareas inÃºtiles.  
- **Equipo de Desarrollo**: Mete mano al cÃ³digo, mejora el pipeline y mantiene todo andando.  
- **Stakeholders (Usuarios del pipeline)**: Dan feedback sobre que funciona y que no.  

### **ğŸ”¹ Eventos en Scrum:**  
- **Daily Standup:** 15 min pa ver avances y si hay bloqueos.  
- **Sprint Planning:** Se define que se hace en las prÃ³ximas 2 semanas podemos usar History points con fibonaci para mejorar la planificaciÃ³n.  
- **Sprint Review:** Se presenta lo que se terminÃ³ a los interesados.  
- **Sprint Retro:** Se analiza que mejorar pa la siguiente ronda.  

---

## **PriorizaciÃ³n con la Matriz de Eisenhower**  

Pa que no nos llenemos de tareas sin sentido, usamos la **Matriz de Eisenhower**, que bÃ¡sicamente nos dice **quÃ© hacer ya**, **quÃ© se puede planear**, **quÃ© se delega** y **quÃ© se manda a la basura**.

### **ğŸ”¹ Ejemplos de quÃ© cae en cada categorÃ­a**:  

- **Urgente & Importante:**  
  - Airflow dejÃ³ de ejecutar DAGs â†’ **Se arregla ya mismo.**  
  - Fallo en Glue que rompe el pipeline â†’ **No se espera, se arregla.**  

- **No Urgente pero Importante:**  
  - Optimizar tiempos en Airflow â†’ **Se mete en el prÃ³ximo sprint.**  
  - Refactorizar cÃ³digo pa que sea mÃ¡s mantenible â†’ **Se agenda.**  

- **Urgente pero No Importante:**  
  - Analista pide ayuda con una consulta SQL â†’ **Se delega al equipo de BI.**  
  - Soporte a usuarios por una duda simple â†’ **Se responde cuando se pueda.**  

- **Ni Urgente ni Importante:**  
  - Probar una tecnologÃ­a nueva solo por curiosidad â†’ **Se guarda pa despuÃ©s.**  
  - Un usuario quiere un dashboard experimental sin necesidad real â†’ **Baja prioridad.**  

---

## **Conclusion**  
Con **Scrum**, nos organizamos sin volarnos la cabeza con tareas al azar.  
Con **Eisenhower**, no perdemos tiempo en cosas que no aportan nada.  
El **LÃ­der TÃ©cnico/LÃ­der de Datos** se encarga de que el codigo y la arquitectura no se vuelvan un desastre.  
AsÃ­, se trabaja bien sin que cada semana parezca un incendio nuevo. 

---

## **GrÃ¡fico de la Matriz de Eisenhower**  
![alt text](Eisenhower.png)


## ConclusiÃ³n

Esta propuesta presenta un **pipeline de datos moderno y eficiente** que abarca desde la **ingesta en S3**, pasando por un **ETL con AWS Glue y Apache Hudi**, hasta la **carga en un data warehouse y el anÃ¡lisis con herramientas de BI**.

La **automatizaciÃ³n del despliegue con Terraform y GitHub Actions**, combinada con el uso de **TDD, SOLID y Clean Architecture**, asegura una soluciÃ³n robusta, escalable y adaptable a futuras necesidades. Este enfoque es la base ideal para proyectos de ingenierÃ­a de datos que requieren alta calidad, eficiencia y capacidad de evoluciÃ³n en entornos dinÃ¡micos.
