# Modelo Operativo para LD-nequi-data-pipeline

## 1. Introducci√≥n

Este modelo operativo define c√≥mo vamos a manejar el pipeline de datos, asegurando que todo funcione de forma autom√°tica, segura y con alta calidad. Desde que llegan los archivos a S3 hasta que los analizamos en herramientas de BI. Se basa en pr√°cticas como **TDD, SOLID y Clean Architecture**, y el uso de **CI/CD** para los despliegues.

## 2. Componentes del Modelo Operativo

### 2.1. Ingesta y Trigger
- **Qu√© pasa**:  
  - Se configuran **hooks en Airflow** para extraer datos desde bases de datos **SQL**.  
  - Se programan queries automatizadas que extraen y cargan los datos a un **bucket S3** o los procesan directamente en AWS Glue.  
  - Tambi√©n se cargan archivos **CSV** en un bucket de **S3** desde fuentes externas.  
  - La llegada de archivos o la ejecuci√≥n programada de queries dispara un DAG en **Airflow**.  

- **Or√≠genes de datos**:  
  - Bases de datos relacionales (**PostgreSQL, MySQL, SQL Server, Oracle**).  
  - Archivos en formato **CSV, JSON o Parquet** desde sistemas externos.  
  - APIs o servicios de terceros que env√≠an datos en tiempo programado.  

- **Qui√©n lo cuida**:  
  - El equipo de **DataOps**, que monitorea la ingesta de datos y mantiene los pipelines funcionando correctamente.  

- **M√©tricas a seguir**:  
  - **Tiempo de ingesta**: cu√°nto tarda en extraerse y cargarse la data.  
  - **Porcentaje de √©xito**: cantidad de archivos o queries completadas vs. fallidas.  
  - **Volumen de datos**: cantidad de registros procesados por ejecuci√≥n.  


### 2.2. Procesamiento y Curado
- **Qu√© pasa**:  
  - **Airflow** orquesta jobs en **AWS Glue**.  
  - **PySpark** procesa los CSV, aplicando transformaciones, validaciones y curado con **Apache Hudi**.  
- **Qui√©n lo cuida**:  
  - Equipo de ETL / Ingenier√≠a de datos.  
- **M√©tricas a seguir**:  
  - Tiempo de procesamiento.  
  - Tasa de √©xito en las transformaciones.  
  - Versionado correcto de los datos con Hudi.  

### 2.3. Notificaciones y Monitoreo
- **Qu√© pasa**:  
  - Alertas autom√°ticas v√≠a **SES, Slack y Teams**.  
  - **CloudWatch y Grafana** para logs y m√©tricas.  
- **Qui√©n lo cuida**:  
  - DevOps / Operaciones.  
- **M√©tricas a seguir**:  
  - Tiempo de respuesta ante incidentes.  
  - N√∫mero de alertas cr√≠ticas resueltas.  

### 2.4. Almacenamiento y An√°lisis
- **Qu√© pasa**:  
  - Datos en **S3** y carga en **Snowflake** si es necesario.  
  - Conexi√≥n con herramientas de BI como **QuickSight** o **Grafana**.  
- **Qui√©n lo cuida**:  
  - Equipo de BI y Data Analysts.  
- **M√©tricas a seguir**:  
  - Latencia en la carga a Snowflake.  
  - Calidad de los dashboards generados.  

### 2.5. CI/CD y Despliegue
- **Qu√© pasa**:  
  - **Terraform** maneja la infraestructura.  
  - **GitHub Actions** ejecuta validaciones cuando hay cambios en el c√≥digo.  
  - Flujo de revisi√≥n con Pull Requests y code review.  
- **Qui√©n lo cuida**:  
  - DevOps y desarrollo.  
- **M√©tricas a seguir**:  
  - Tiempo de despliegue.  
  - Cobertura de pruebas automatizadas.  

## 3. Roles y Responsabilidades

- **L√≠der de Datos**: Supervisa, revisa y aprueba el c√≥digo.  
- **Desarrolladores/Ingenieros de Datos**: Desarrollan y mantienen el pipeline.  
- **DevOps/Operaciones**: Gestionan la infraestructura y monitoreo.  
- **Equipo de BI**: Crean y mantienen dashboards y an√°lisis.  

## 4. Procesos y Flujos de Trabajo

### 4.1. Flujo de Datos
1. **Ingesta**: Llega un archivo a S3 ‚Üí Se dispara un DAG en Airflow.  
2. **Procesamiento**: Airflow ejecuta Glue ‚Üí PySpark transforma y cura los datos con Hudi.  
3. **Carga y An√°lisis**: Datos almacenados en S3 ‚Üí Se cargan en Snowflake ‚Üí Se analizan con BI.  

### 4.2. Flujo de Desarrollo y Despliegue
1. **Commit & Push**: El desarrollador sube cambios.  
2. **Pull Request & CI/CD**: Se ejecutan tests y validaciones.  
3. **Code Review**: El l√≠der revisa y aprueba o pide cambios.  
4. **Merge & Despliegue**: Se integran los cambios y se despliegan con Terraform.  

## 5. Gesti√≥n de Incidencias y Soporte

- **Plan de Respuesta**: Se definen tiempos y responsables por tipo de incidente.  
- **Herramientas**: CloudWatch y alertas en Slack para monitoreo en tiempo real.  
- **Registro de Incidentes**: Documentaci√≥n de cada fallo y mejoras postmortem.  

## 6. Mantenimiento y Mejora Continua

- **Revisiones Peri√≥dicas**: Se analizan logs y m√©tricas para optimizar procesos.  
- **Capacitaciones**: Entrenamientos regulares para el equipo.  
- **Feedback y Ajustes**: Se toman acciones con base en m√©tricas y retroalimentaci√≥n.  

## 7. Seguridad y Cumplimiento

- **Cifrado de Datos**: Encriptaci√≥n en reposo y en tr√°nsito.  
- **Gesti√≥n de Accesos**: Roles y permisos en AWS IAM con privilegios m√≠nimos.  
- **Auditor√≠a**: CloudTrail y AWS Config para seguimiento de cambios.  

## 8. Documentaci√≥n y Capacitaci√≥n

- **Repositorio de Documentaci√≥n**: En el directorio `docs/` (diagramas, manuales, etc.).  
- **Capacitaciones**: Formaci√≥n peri√≥dica para nuevos integrantes.  
- **Protocolos Documentados**: Gu√≠as de operaci√≥n, despliegue y manejo de incidencias.  



## 9 Gesti√≥n del Pipeline con Scrum y Priorizaci√≥n Eisenhower

Pa organizar bien el trabajo en el pipeline usamos **Scrum** (para que el equipo tenga claro que hacer en cada sprint) y la **Matriz de Eisenhower** (para no perder tiempo en cosas que no aportan valor real y enfocarnos en lo que de verdad importa).

## üîπ **Scrum en el Pipeline**  

La idea es trabajar en **sprints de 2 semanas**, asi aseguramos que siempre hay avances sin meter cambios a lo loco que rompan todo.

### **üîπ Roles en Scrum:**  
- **Product Owner (PO)**: Define que se hace en cada sprint y que es lo m√°s importante pa el negocio.  
- **L√≠der T√©cnico / L√≠der de Datos**:  
  - Se encarga de que la arquitectura y el c√≥digo sean solidos.  
  - Revisa Pull Requests y asegura que todo pase los tests y validaciones.  
  - Evita que se hagan cosas sin sentido y que el equipo no pierda tiempo en tareas in√∫tiles.  
- **Equipo de Desarrollo**: Mete mano al c√≥digo, mejora el pipeline y mantiene todo andando.  
- **Stakeholders (Usuarios del pipeline)**: Dan feedback sobre que funciona y que no.  

### **üîπ Eventos en Scrum:**  
- **Daily Standup:** 15 min pa ver avances y si hay bloqueos.  
- **Sprint Planning:** Se define que se hace en las pr√≥ximas 2 semanas podemos usar History points con fibonaci para mejorar la planificaci√≥n.  
- **Sprint Review:** Se presenta lo que se termin√≥ a los interesados.  
- **Sprint Retro:** Se analiza que mejorar pa la siguiente ronda.  

---

## **Priorizaci√≥n con la Matriz de Eisenhower**  

Pa que no nos llenemos de tareas sin sentido, usamos la **Matriz de Eisenhower**, que b√°sicamente nos dice **qu√© hacer ya**, **qu√© se puede planear**, **qu√© se delega** y **qu√© se manda a la basura**.

### **üîπ Ejemplos de qu√© cae en cada categor√≠a**:  

- **Urgente & Importante:**  
  - Airflow dej√≥ de ejecutar DAGs ‚Üí **Se arregla ya mismo.**  
  - Fallo en Glue que rompe el pipeline ‚Üí **No se espera, se arregla.**  

- **No Urgente pero Importante:**  
  - Optimizar tiempos en Airflow ‚Üí **Se mete en el pr√≥ximo sprint.**  
  - Refactorizar c√≥digo pa que sea m√°s mantenible ‚Üí **Se agenda.**  

- **Urgente pero No Importante:**  
  - Analista pide ayuda con una consulta SQL ‚Üí **Se delega al equipo de BI.**  
  - Soporte a usuarios por una duda simple ‚Üí **Se responde cuando se pueda.**  

- **Ni Urgente ni Importante:**  
  - Probar una tecnolog√≠a nueva solo por curiosidad ‚Üí **Se guarda pa despu√©s.**  
  - Un usuario quiere un dashboard experimental sin necesidad real ‚Üí **Baja prioridad.**  

---

## **Conclusion**  
Con **Scrum**, nos organizamos sin volarnos la cabeza con tareas al azar.  
Con **Eisenhower**, no perdemos tiempo en cosas que no aportan nada.  
El **L√≠der T√©cnico/L√≠der de Datos** se encarga de que el codigo y la arquitectura no se vuelvan un desastre.  
As√≠, se trabaja bien sin que cada semana parezca un incendio nuevo. 

---

## **Gr√°fico de la Matriz de Eisenhower**  
![alt text](Eisenhower.png)

## 10. Conclusi√≥n

Este modelo operativo nos permite tener control total sobre el pipeline, asegurando que todo fluya sin problemas, con buenas pr√°cticas, monitoreo y automatizaci√≥n. La clave est√° en la mejora continua, con revisiones regulares y optimizaci√≥n en cada etapa.

